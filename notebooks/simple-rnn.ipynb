{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll train the simple RNN model that can be found in the `src/models` directory and use a few of the functions provided by neural-seq to process the midi files. For this we'll train it on a single Phil Collins song. If the model has been implemented correctly then we should see the loss decrease and the model should essentially memorise the performance. This is a useful sanity check, but also it's interesting to play around with the sampling strategy; even though an RNN that has memorised a single track isn't all that academically interesting, by adding some stochastisity to the generated output it demonstrates some potential as a creative tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.midi_encode import MIDIData\n",
    "from src.encoders.simple_drum import DrumEncoder\n",
    "from src.utils.midi_data import fetch_midi_data, play_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding MIDI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch the dataset first. This will download the [The Lakh MIDI](https://colinraffel.com/projects/lmd/) which includes the song we're after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_midi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mA Groovy Kind of Love.mid\u001b[m\u001b[m*     \u001b[31mIn The Air Tonight.1.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mAgainst All Odds.mid\u001b[m\u001b[m*          \u001b[31mIn The Air Tonight.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mAnother Day in Paradise.1.mid\u001b[m\u001b[m* \u001b[31mNo Son of Mine.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mAnother Day in Paradise.2.mid\u001b[m\u001b[m* \u001b[31mOne More Night.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mAnother Day in Paradise.mid\u001b[m\u001b[m*   \u001b[31mSussudio.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mDon't Lose My Number.mid\u001b[m\u001b[m*      \u001b[31mTrue Colors.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mEasy Lover.mid\u001b[m\u001b[m*                \u001b[31mYou Can't Hurry Love.mid\u001b[m\u001b[m*\r\n",
      "\u001b[31mI Wish It Would Rain Down.mid\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../.data/midi_data/clean_midi/Phil\\ Collins/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify an encoder. The `simple_drum` encoder will convert the MIDI file (supplied to it as a [PrettyMIDI](http://craffel.github.io/pretty-midi/) object) into a list of 'action' and 'duration' tokens that specify the pitch (the drum in this case) that is played and the duration until the next set of pitches is played. This is essentially a polyphonic encoding scheme but since the expected instrument is percussive and the sounds are short-lived, all the lengths are fixed to a 32nd-note duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DrumEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the songs that we're going to encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [\n",
    "    'In The Air Tonight.mid',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_songs(songs, encoder):\n",
    "    encodings = []\n",
    "    for s in songs:\n",
    "        midi_data = MIDIData.from_file(f'../.data/midi_data/clean_midi/Phil Collins/{s}', encoder, instrument='drum kit')\n",
    "        encodings += (midi_data.encode())\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that an instrument filter was applied when loading the MIDI data which filtered out the drum kit. Any of the names from the [General MIDI Instrument list](https://soundprogramming.net/file-formats/general-midi-instrument-list/) can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found instrument matching filter: Drum Kit\n"
     ]
    }
   ],
   "source": [
    "encoded = encode_songs(songs, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the tokens that are in the encoded performance we can extract the vocabulary (the unique tokens that make up the encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(encoded))\n",
    "vocab_idx = {char: i for i, char in enumerate(vocab)}\n",
    "vocab_sz = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first ten tokens look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P-42,D-8,P-42,D-8,P-42,D-8,P-42,D-8,P-46,D-4'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll construct a simple RNN model (this is essentially the same as the one included in `src/models`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz=300, n_hid=600, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz)\n",
    "        self.lstm = nn.LSTM(input_size=emb_sz, hidden_size=n_hid, num_layers=n_layers)\n",
    "        self.decoder = nn.Linear(n_hid, vocab_sz)\n",
    "        \n",
    "    def forward(self, mb, hidden=None):\n",
    "        x = self.encoder(mb)\n",
    "        x, hidd = self.lstm(x, hidden)\n",
    "        return self.decoder(x), hidd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will take the encoding of the Phil Collins drum track, chop it up into sequences of length 64 and create a target dataset (the following token for each position). The `get_batches` function will divide this up into minibatches that we can then pass into the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(encoding, bptt=64):\n",
    "    encoding = [vocab_idx[enc] for enc in encoding]\n",
    "    seqs = []\n",
    "    targs = []\n",
    "    for i in range(len(encoding)//(bptt + 1)):\n",
    "        seqs.append(encoding[i*bptt:(i+1)*bptt])\n",
    "        targs.append(encoding[(i*bptt)+1:((i+1)*bptt) + 1])\n",
    "    return torch.LongTensor(seqs).permute((1, 0)), torch.LongTensor(targs).permute((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs, targets, bs=64):\n",
    "    batches = []\n",
    "    for i in range(inputs.shape[1]//32):\n",
    "        batches.append((inputs[:, i*bs:(i+1)*bs], targets[:, i*bs:(i+1)*bs]))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure below will train the model for 25 epochs using an Adam optimiser. The loss is calculated using the cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 2.7856833934783936\n",
      "epoch 1 loss 2.0310475826263428\n",
      "epoch 2 loss 1.5986616611480713\n",
      "epoch 3 loss 1.3271511793136597\n",
      "epoch 4 loss 1.1650668382644653\n",
      "epoch 5 loss 1.0346001386642456\n",
      "epoch 6 loss 0.92838054895401\n",
      "epoch 7 loss 0.8601027727127075\n",
      "epoch 8 loss 0.775689959526062\n",
      "epoch 9 loss 0.7118246555328369\n",
      "epoch 10 loss 0.6587616205215454\n",
      "epoch 11 loss 0.5966862440109253\n",
      "epoch 12 loss 0.5482185482978821\n",
      "epoch 13 loss 0.5103653073310852\n",
      "epoch 14 loss 0.4751216769218445\n",
      "epoch 15 loss 0.4349558353424072\n",
      "epoch 16 loss 0.396340936422348\n",
      "epoch 17 loss 0.3698026239871979\n",
      "epoch 18 loss 0.33964991569519043\n",
      "epoch 19 loss 0.3158837556838989\n",
      "epoch 20 loss 0.29283303022384644\n",
      "epoch 21 loss 0.26924362778663635\n",
      "epoch 22 loss 0.25207337737083435\n",
      "epoch 23 loss 0.2305152416229248\n",
      "epoch 24 loss 0.21670326590538025\n"
     ]
    }
   ],
   "source": [
    "x, y = create_dataset(encoded)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=3e-3)\n",
    "for i in range(25):\n",
    "    epoch_loss = 0\n",
    "    num = 0\n",
    "    for xb, yb in get_batches(x, y):        \n",
    "        rnn.zero_grad()\n",
    "        out, _, _ = rnn(x)\n",
    "        loss = F.cross_entropy(out.reshape(-1, vocab_sz), y.reshape(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        num += 1\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch {i} loss {epoch_loss/num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate continuations for a given prompt! In order to do this we'll\n",
    "\n",
    "1. Convert the prompt (which is at least semi human-readable) into a model input of suitable type and dimension, i.e. of size (seq, batch_size=1)\n",
    "\n",
    "2. Feed in the input to the model and perform [nucleus sampling](https://arxiv.org/abs/1904.09751). This involves collection the most probable continuations that sum up to a given threshold (0.9 in the example below), renormalising the resulting selection and then sampling from it.\n",
    "\n",
    "3. Feed in the resulting prompt again until we reach the specified duration (256 32nd notes, or 8 bars, in the example below). Notice that the `encoder.duration` method is used to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature (`temp`) for the initial softmax can be twiddled with to flatten out the distribution, making other tokens more likely or make it more confident in its prediction thus reducing stochasticity. A value of 1 is quite conservative whereas 1.5 results some... interesting beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(prompt):\n",
    "    seq = [vocab_idx[sym] for sym in prompt]\n",
    "    return torch.LongTensor(seq).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.3\n",
    "with torch.no_grad():\n",
    "    prompt = ['P-42', 'D-8']\n",
    "    hidd = None\n",
    "    while encoder.duration(prompt) < 256:\n",
    "        seq = convert_to_input(prompt)\n",
    "        out, hidd = rnn(seq[-1].unsqueeze(-1), hidd)\n",
    "        # Get the last vector of logprobs\n",
    "        logprobs = out.reshape(-1, vocab_sz)[-1]\n",
    "                \n",
    "        nucleus_probs = []\n",
    "        nucleus_indices = []\n",
    "        \n",
    "        sorted_probs = F.softmax(logprobs/temp, -1).sort(descending=True)\n",
    "        for p, idx in zip(sorted_probs[0], sorted_probs[1]):\n",
    "            nucleus_probs.append(p)\n",
    "            nucleus_indices.append(idx)\n",
    "            if sum(nucleus_probs) > 0.9:\n",
    "                break\n",
    "\n",
    "        unnormalised = torch.Tensor(nucleus_probs)\n",
    "        probs =  unnormalised * (1/sum(torch.Tensor(unnormalised)))\n",
    "        # We need to refer back to the original indices to grab the correct vocab elements\n",
    "        prediction = nucleus_indices[torch.distributions.Categorical(probs).sample().item()]\n",
    "        prompt.append(vocab[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P-42', 'D-8', 'P-64', 'P-50', 'P-42', 'D-8', 'P-42', 'P-41', 'P-35', 'D-4']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can listen to the generated output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing... (Ctrl+C to stop)\n"
     ]
    }
   ],
   "source": [
    "play_midi(encoder.decode(prompt, tempo=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
